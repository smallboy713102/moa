{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb63119",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = 'sk-....'\n",
    "api_key='........'\n",
    "import os\n",
    "os.environ['GROQ_API_KEY'] = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62df891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: add tracing to visualize the agent trajectories\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0f32b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Any, List, Dict\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.utilities.tavily_search import TavilySearchAPIWrapper\n",
    "import json \n",
    "class State(TypedDict):\n",
    "    input: str\n",
    "    responses: str\n",
    "    feedback: str\n",
    "    aggregated_response: Annotated[list, operator.add]\n",
    "    final_answer: str\n",
    "    iteration: int\n",
    "    max_iterations: int\n",
    "    reflection: str\n",
    "\n",
    "# Initialize models and search tool\n",
    "proposer_models = [\n",
    "    ChatGroq(temperature=0, model_name=\"llama3-70b-8192\",),\n",
    "    ChatGroq(temperature=0, model_name=\"llama3-8b-8192\",),\n",
    "    ChatGroq(temperature=0, model_name=\"mixtral-8x7b-32768\"),\n",
    "]\n",
    "\n",
    "aggregator_model = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "reflector_model = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfeb8ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Proposer:\n",
    "    def __init__(self, model: ChatOpenAI, id: str):\n",
    "        self.model = model\n",
    "        self.id = id\n",
    "\n",
    "    def __call__(self, state: State) -> Dict[str, Dict[str, str]]:\n",
    "        \n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"human\", \"Please provide a response to the following prompt: {input}\\n\\n\"\n",
    "             \"The previous response was {response}\\n\\n\"\n",
    "             \"The provided feedbackw was {feedback} \\n\\n\"\n",
    "                      \"Always answer with as much detail as possible.\")\n",
    "        ])\n",
    "        response = self.model(prompt.format_messages(\n",
    "            input=state[\"input\"],\n",
    "            response=state['responses'],\n",
    "            feedback=state['feedback']\n",
    "            \n",
    "        ))\n",
    "        \n",
    "        \n",
    "        \n",
    "        return {\"aggregated_response\": [json.dumps({self.id: response.content})]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac10bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregator(state: State) -> State:\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        #(\"human\", \"Given the following responses to the prompt '{input}', please synthesize them into a single, high-quality response:\\n\\n{responses}\")\n",
    "        (\"human\", '''\n",
    "        You have been provided with a set of responses from various open-source models to the  user query '{input}'.\n",
    "        Your task is to synthesize these responses into a single, high-quality response.\n",
    "        It is crucial to critically evaluate the information provided in these responses, \n",
    "        recognizing that some of it may be biased or incorrect. \n",
    "        Your response should not simply replicate the given answers but should offer a refined, accurate, and comprehensive reply to the instruction. Ensure your response is well-structured, coherent, and adheres to the highest standards of accuracy and reliability.\n",
    " \n",
    "        Responses from models:\n",
    "        \\n\\n{responses}\n",
    "\n",
    "''')\n",
    "    ])\n",
    "    response = aggregator_model(prompt.format_messages(\n",
    "        input=state[\"input\"],\n",
    "        responses=\"\\n\\n\".join(state[\"aggregated_response\"])\n",
    "    ))\n",
    "    state[\"responses\"] = response.content\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809cb2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "\n",
    "class GradeGeneration(BaseModel):\n",
    "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "    score: str = Field(\n",
    "        description=\"Is this the correct answer to the question, 'yes' or 'no'\"\n",
    "    )\n",
    "    feedback: str = Field(\n",
    "        description=\"Provided specific feedback for improvement\"\n",
    "    )\n",
    "        \n",
    "def reflector(state: State) -> State:\n",
    "    parser = PydanticOutputParser(pydantic_object=GradeGeneration)\n",
    "\n",
    "    prompt  = ChatPromptTemplate.from_messages([\n",
    "            (\"human\", \"Given the following answer to the question: '{input}'\\n\\n\"\n",
    "            \"Answer: {aggregated_response}\\n\\n\"\n",
    "                      \"If the answer is satisfactory and complete, grade it as yes. \\n\"\n",
    "    \"Provide json object with  feedback string and binary score 'yes' or 'no' score to indicate whether the answer is correct\"\n",
    "            )\n",
    "        ])\n",
    "    chain = prompt | reflector_model | parser\n",
    "\n",
    "    response = chain.invoke({\n",
    "        'input':state[\"input\"],\n",
    "        'aggregated_response':state[\"responses\"]\n",
    "    })\n",
    "    state[\"reflection\"] = response.score\n",
    "    state['feedback'] = response.feedback\n",
    "    state[\"iteration\"] += 1\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042dc3d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46c6fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def should_continue(state: State) -> str:\n",
    "    if state[\"iteration\"] >= state[\"max_iterations\"]:# or \"YES\" in state[\"reflection\"].upper():\n",
    "        state[\"final_answer\"] = state[\"responses\"]\n",
    "        return \"end\"\n",
    "    return \"refine\"\n",
    "\n",
    "# Create the graph\n",
    "workflow = StateGraph(State)\n",
    "\n",
    "# Add nodes\n",
    "for i, model in enumerate(proposer_models):\n",
    "    workflow.add_node(f\"proposer_{i}\", Proposer(model, f\"proposer_{i}\"))\n",
    "workflow.add_node(\"aggregator\", aggregator)\n",
    "workflow.add_node(\"reflector\", reflector)\n",
    "\n",
    "# Define edges\n",
    "workflow.set_entry_point(\"proposer_0\")\n",
    "for i in range(1, len(proposer_models)):\n",
    "    workflow.add_edge(f\"proposer_0\", f\"proposer_{i}\")\n",
    "    workflow.add_edge(f\"proposer_{i}\", \"aggregator\")\n",
    "workflow.add_edge(\"aggregator\", \"reflector\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"reflector\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"refine\": \"proposer_0\",\n",
    "        \"end\": END\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b6076e",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29abd888",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175ffb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_moa(question):\n",
    "    # Run the graph\n",
    "    initial_state = {\n",
    "        \"input\": question,\n",
    "        \"responses\": '',\n",
    "        'feedback':\"\",\n",
    "        \"aggregated_response\": [],\n",
    "        \"final_answer\": \"\",\n",
    "        \"iteration\": 0,\n",
    "        \"max_iterations\": 3,\n",
    "        \"reflection\": \"\"\n",
    "    }\n",
    "    for output in app.stream(initial_state):\n",
    "        for key, value in output.items():\n",
    "            print(f\"Finished running: {key}:\")\n",
    "    print(value['responses'])\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44358781",
   "metadata": {},
   "outputs": [],
   "source": [
    "value = query_moa(\"Explain the latest advancements in quantum computing and their potential applications.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56d64e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "value=query_moa(\"Write the snake game in python without any errors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5514bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "value = query_moa(\"if we lay 5 shirts in the sun and it takes 4 hours to dry, how long would 20 shirts take to dry?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab28d33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3ed9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "value = query_moa('''Maria is staying at a hotel that charges $99.95 per night plus tax for a room. A tax of 8% is applied to the room rate, \n",
    "and an additional onetime untaxed fee of $5.00 is charged by the hotel. \n",
    "Which of the following represents Mariaâ€™s total charge, in dollars, for staying x nights?  (99.95 + 0.08x) + 5  1.08(99.95x) + 5  1.08(99.95x + 5)  1.08(99.95 + 5)x''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22892a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "value =  query_moa('''There are three killers in a room.\n",
    "Someone enters the room and kills one of them. Nobody leaves the room. How many killers are left in the room? Explain your reasoning step by step.''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e0dba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "value =  query_moa('''A marble is put in a glass.\n",
    "The glass is then turned upside down and put on a table.\n",
    "Then the glass is picked up and put in a microwave. \n",
    "Where's the marble? Explain your reasoning step by step.''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb12eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "value = query_moa('Give me 10 sentences that end in the word Apple.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cfb6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "value = query_moa('''It takes one person 5 hours to dig a 10 foot hole in the ground. \n",
    "How long would it take 50 people to dig a single 10 foot hole?'\n",
    "\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb77455f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
